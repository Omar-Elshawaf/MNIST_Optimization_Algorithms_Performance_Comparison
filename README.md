# MNIST_Optimization_Algorithms_Performance_Comparison
This project compares the performance of different optimization algorithms (SGD, Momentum, Adagrad, Adam) in training a shallow neural network on the MNIST dataset. The focus is on evaluating training time and classification accuracy.
